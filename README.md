# BERT
Bidirectional Encoder Representations from Transformers

|  [PAPER](https://arxiv.org/abs/1810.04805)  |  [Hugging Face](https://arxiv.org/abs/1810.04805)  |  

The BERT model was proposed in **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. Itâ€™s a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.

## How to see ðŸ˜²

**Just type the '.' on your keyboard now!!**, 

![Inkedkeyb_LI](https://user-images.githubusercontent.com/46081500/157036957-1af65660-cf8d-4f03-891d-4d951d88d861.jpg)


for watching the directory (codes) on Visual Studio Code -Github

 
![log-git](https://user-images.githubusercontent.com/46081500/157036461-226dca5a-03a7-41dc-a821-e85c1189089b.PNG)
