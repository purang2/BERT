# BERT
Bidirectional Encoder Representations from Transformers



|  [PAPER (theory)](https://arxiv.org/abs/1810.04805)  |  [Hugging Face (engineering) ](https://huggingface.co/docs/transformers/model_doc/bert)  |  [![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fpurang2%2FBERT&count_bg=%2373EC9A&title_bg=%23555555&icon=github.svg&icon_color=%23EAE2E2&title=hits&edge_flat=false)](https://hits.seeyoufarm.com) |
  
The BERT model was proposed in **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. Itâ€™s a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.

## How to see ðŸ˜²

**Just type the '.' on your keyboard now!!**, 

![Inkedkeyb_LI](https://user-images.githubusercontent.com/46081500/157036957-1af65660-cf8d-4f03-891d-4d951d88d861.jpg)


for watching the directory (codes) on Visual Studio Code -Github

 
![log-git](https://user-images.githubusercontent.com/46081500/157036461-226dca5a-03a7-41dc-a821-e85c1189089b.PNG)
